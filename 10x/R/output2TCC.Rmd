---
title: "Loading kallisto output"
author: "Lambda Moses"
date: "11/12/2018"
output: html_document
---

In this notebook, I convert the output from kallisto bus into a matrix with gene expression in each cell that can be used for downstream analysis.
```{r, message=FALSE}
library(data.table)
library(Matrix)
library(Biostrings)
library(stringr)
library(zeallot)
library(parallel)
library(Rcpp)
library(Seurat)
library(loomR)
ncores <- detectCores()
# Get the C++ function that converts kallisto output into matrix
sourceCpp("fill_cell_gene.cpp")
```

## From equivalence classes to genes
```{r}
# Load equivalence classes
ECs <- fread("../../matrix.ec", header = FALSE, col.names = c("EC_index", "EC"))
```

```{r}
# Get the 10x whitelist for whitelistetd barcodes
whitelist <- fread("../../10xwhitelist.txt", header = FALSE)$V1
```

How do I get from ECs to genes? The number in each EC is the line number of the transcript compatible with the read in the index built in the first step of running kallisto. Here while I didn't run kallisto on this dataset myself, I know which transcriptomes were used for the index. In this dataset, 

```{r, eval=FALSE}
# Download the transcriptomes
download.file("ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz", "hs_cdna.fa.gz")
download.file("ftp://ftp.ensembl.org/pub/release-94/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz", "mm_cdna.fa.gz")
```

```{r, eval = FALSE}
hs_cdna <- readDNAStringSet("hs_cdna.fa.gz")
mm_cdna <- readDNAStringSet("mm_cdna.fa.gz")
```

```{r, eval = FALSE}
# Extract gene and transcript IDs from fasta line names
names2genes <- function(cdna, species = "human") {
  tr_regex <- switch (species,
    "human" = "^ENST[\\d.]*",
    "mouse" = "^ENSMUST[\\d.]*"
  )
  gene_regex <- switch(species,
    "human" = "ENSG[\\d.]*",
    "mouse" = "ENSMUSG[\\d.]*")
  data.frame(transcript = str_extract(names(cdna), tr_regex),
             gene = str_extract(names(cdna), gene_regex),
             stringsAsFactors = FALSE)
}
```

```{r, eval = FALSE}
# Get the transcript and gene IDs
tr2g_hs <- names2genes(hs_cdna)
tr2g_mm <- names2genes(mm_cdna, species = "mouse")
tr2g <- rbind(tr2g_hs, tr2g_mm)
fwrite(tr2g, "tr2g")
```

```{r}
# Load the results; not running the same thing again.
tr2g <- fread("../../tr2g")
```

```{r}
# Map each EC to gene
genes <- tr2g$gene
ECs[, c("EC_index", "EC") := .(EC_index, 
                               str_split(EC, ","))]
ECs[, genes := mclapply(EC, 
                        function(x) {
                          inds <- as.integer(x) + 1
                          unique(genes[inds])
                        }, mc.cores = ncores)]
ECs[, EC := NULL]
```

How many distinct genes are present? 
```{r}
length(unique(unlist(ECs$genes)))
```

## From output to matrix
What that Python script does is that for each barcode and each UMI, find the genes corresponding to the EC for that UMI, and if the same UMI appeared multiple times for the same barcode, the intersection of the corresponding genes is taken. Ideally, since the UMI identifies an mRNA, even if different reads with the same UMI map to multiple genes, the list of genes they map to should overlap since those reads are in fact from the same gene. If a UMI maps to multiple genes, then each of those gene gets an equal share of the UMI count. Later, for the same barcode, suppose a different UMI generated a set of genes that includes a gene that is already present in `cell_gene`, then that gene gets more share of the UMI. 

```{r}
fill_cell_geneR <- function(fn, genes, whitelist, est_ncells, est_ngenes, display_progress = TRUE) {
  if (!file.exists(fn)) {
    stop(paste("The file", fn, "doesn't exist."))
  }
  c(res_mat, barcodes, genes) %<-% fill_cell_gene(fn, genes, whitelist, est_ncells, est_ngenes, display_progress)
  rownames(res_mat) <- genes
  colnames(res_mat) <- barcodes
  res_mat
}
```

```{r}
Sys.time()
res_mat <- fill_cell_geneR("../../output.sort.txt", ECs$genes, whitelist = whitelist, est_ncells = 7e5, est_ngenes = 8e4)
Sys.time()
```
For the hgmm6k dataset, which has about 280 million lines in the `output.sort.txt` file (which is over 9 GB) and about 6.7e5 cells, it took about 6 minutes to get the sparse matrix indices ready (for the C++ function to execute), and a little more to construct the sparse matrix in R, so in total it takes about 6 minutes and 40 seconds. I also tried directly returning a sparse matrix from the C++ function, and the time taken is about the same, and I'll need to add the column names and row names later. Perhaps I can further speed up the code by multithreading and improving my C++ fluency.
```{r}
# Check that the whitelist is enforced
all(colnames(res_mat) %in% whitelist)
```

```{r}
# Check that all genes are detected in at least one cell
all(Matrix::rowSums(res_mat) > 0)
```

```{r}
# number of UMIs per cell
tot_umi <- Matrix::colSums(res_mat)
summary(tot_umi)
```

```{r}
# total number of UMIs for each gene
summary(Matrix::rowSums(res_mat))
```

```{r}
# A histogram of UMIs per cell in cells without too many reads
hist(tot_umi[tot_umi <= 500], breaks = 100, main = "Histogram of number of UMI per cell")
```

## Data analysis (with Seurat object)
Here I use Seurat for some basic exploratory data analysis. If you're new to Seurat, see [these vignettes](https://satijalab.org/seurat/get_started.html) for a more thorough introduction.
```{r}
seu <- CreateSeuratObject(res_mat) %>% 
  NormalizeData() %>% FindVariableGenes()
```

```{r}
# Basic QC
VlnPlot(seu, features.plot = c("nGene", "nUMI"))
```

You should adjust the x and y cutoffs in `FindVariableGenes` if you want more or fewer highly variable genes.
```{r}
seu <- FindVariableGenes(seu, y.cutoff = 1)
# How many highly variable genes
length(seu@var.genes)
```

```{r}
seu <- RunPCA(seu, do.print = FALSE, pcs.compute = 40, pc.genes = seu@var.genes)
PCElbowPlot(seu, num.pc = 40)
```

```{r}
# Clustering
seu <- FindClusters(seu, resolution = 1)
```

```{r}
PCAPlot(seu)
```

```{r}
seu <- RunTSNE(seu, dims.use = 1:30)
```

```{r}
TSNEPlot(seu)
```

## Data analysis (with loom)
This section is for data exploration of datasets that don't fit into memory. The data format loom stores data on disk in an hdf5 based format, and when working with loom files, only a portion of the data is loaded into memory at a time, thus avoiding the need to force very large datasets into memory. Seurat objects grow very quickly in size as the analysis goes on. 

Please use the loom branch of Seurat for this section, which is different from the version from CRAN. See [this vignette](https://satijalab.org/loomR/loomR_tutorial.html) for an introduction to the R interface to loom file, and see [this vignette](https://satijalab.org/seurat/mca_loom.html) for an introduction to the loom edition of Seurat and how to install it. Note that the loom edition of Seurat is still in development, so you may encounter some bugs. 

```{r, eval = FALSE}
# Write to loom, the first run
hgmm6k <- create("hgmm6k.loom", res_mat)
```

```{r}
# For later runs
hgmm6k <- connect("hgmm6k.loom", "r+")
```

```{r}
NormalizeData(hgmm6k, chunk.size = NULL, chunk.dims = NULL)
FindVariableGenes(hgmm6k, x.low.cutoff = 0.05, y.cutoff = 0.5, overwrite = TRUE)
ScaleData(hgmm6k, overwrite = TRUE, chunk.size = NULL, chunk.dims = NULL)
```

```{r}
RunPCA(hgmm6k, pcs.compute = 75, do.print = FALSE)
```

```{r}
PCElbowPlot(hgmm6k, num.pc = 75)
```

```{r}
PCAPlot(hgmm6k)
```

```{r}
RunTSNE(hgmm6k, dims.use = 1:50)
```

```{r}
TSNEPlot(hgmm6k)
```

